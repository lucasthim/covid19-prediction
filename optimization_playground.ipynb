{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nevergrad as ng\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "\n",
    "import sys\n",
    "sys.path.append('utils')\n",
    "\n",
    "import dataframe_utils\n",
    "import preprocess_utils\n",
    "import model_utils\n",
    "import metrics_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genethic Algorithm\n",
    "# [ ] Drop columns not marked in the bit vector \n",
    "# [ ] Drop rows with less than X% registers (?)\n",
    "# [ ] Drop columns with less than N% of the maximum number of registers (?)\n",
    "# [ ] Drop all NAs\n",
    "# [ ] Drop all columns with more than 99% of equal values\n",
    "# [ ] Train and validate model\n",
    "# [ ] Maximize score function = ( 1 * ((rows * colums) / total datapoints) + 4 * f1_score)/5 \n",
    "\n",
    "# [ ] Use Miss Forest to impute data instead of removing NAs\n",
    "# If using Miss Forest, final score is rows * columns - datapoints imputed\n",
    "\n",
    "\n",
    "# - Drop vectors not selected by the column array\n",
    "# - drop rows with less than X% registers\n",
    "# - Drop all NAs\n",
    "# - Calculate f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset for optmization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('datasets/dataset.xlsx').drop(labels = [\n",
    "        'Patient ID',  \n",
    "        'Patient addmited to intensive care unit (1=yes, 0=no)',\n",
    "        'Patient addmited to semi-intensive unit (1=yes, 0=no)',\n",
    "        'Patient addmited to regular ward (1=yes, 0=no)'],axis = 1)\n",
    "\n",
    "class_label = 'SARS-Cov-2 exam result';\n",
    "empty_features = df.count() == 0\n",
    "df = df.loc[:,empty_features.values == False]\n",
    "df_x = df.drop(labels = [class_label],axis = 1)\n",
    "df_y = df[[class_label]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_to_optimize1(args,verbose = False):\n",
    "\n",
    "    threshold1 = args[0]\n",
    "    threshold2 = args[1]\n",
    "    threshold3 = args[2]\n",
    "\n",
    "    df_subset = drop_by_data_threshold(df,threshold = threshold1, axis = 'column')\n",
    "    df_subset = drop_by_data_threshold(df_subset,threshold = threshold2, axis = 'row')\n",
    "    df_final = drop_by_data_threshold(df_subset,threshold = threshold3, axis = 'column')\n",
    "    df_final.dropna(inplace = True)\n",
    "    \n",
    "    if verbose: print(df_final.shape)\n",
    "    if (df_final.shape[0] * df_final.shape[1]) < 30: return 1\n",
    "    \n",
    "    df_final = encode_features(df_final)\n",
    "    X_train_norm, X_validation_norm, y_train, y_validation = prepare_data_for_model(df_final)\n",
    "    best_model, best_params, best_score = find_best_model(X_train_norm,y_train)\n",
    "\n",
    "    if verbose:\n",
    "        print('shape = ',df_final.shape)\n",
    "        print('f1 = ', best_score);\n",
    "        return best_model, best_params, best_score\n",
    "    \n",
    "    return -best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_to_optimize2(*args,verbose = False):\n",
    "    \n",
    "    inputs = [item for item in args]\n",
    "    \n",
    "    column_selection = inputs[:-1]\n",
    "    row_threshold = inputs[-1]\n",
    "\n",
    "    df_selected = df_x.loc[:,column_selection]\n",
    "    df_selected = pd.concat([df_y, df_selected], axis=1, sort=False)\n",
    "\n",
    "    df_final = drop_by_data_threshold(df_selected,threshold = row_threshold, axis = 'row')\n",
    "    df_final.dropna(inplace = True)\n",
    "\n",
    "    if verbose: print(df_final.shape)\n",
    "    # if df_final.shape[0] == 0 or df_final.shape[1] == 0: return np.inf\n",
    "    if (df_final.shape[0] * df_final.shape[1]) < 30: return 1\n",
    "    \n",
    "    df_final = encode_features(df_final)\n",
    "    X_train_norm, X_validation_norm, y_train, y_validation = prepare_data_for_model(df_final)\n",
    "    best_model, best_params, best_score = find_best_model(X_train_norm,y_train)\n",
    "\n",
    "    if return_model:\n",
    "        print('shape = ',df_final.shape)\n",
    "        print('f1 = ', best_score);\n",
    "        return best_model, best_params, best_score\n",
    "    \n",
    "    return -best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(X_train,y_train):\n",
    "    \n",
    "    logistic_regression_params = {\n",
    "    'C' : [0.1,1,10,100],\n",
    "    'penalty' : ['l2', 'elasticnet'],\n",
    "    'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "    }\n",
    "    score = 'f1'\n",
    "    best_model, best_params, best_score = model_utils.find_best_classification_model_with_cross_validation(\n",
    "        LogisticRegression(random_state=0, class_weight = 'balanced'),\n",
    "        logistic_regression_params,\n",
    "        X_train.values,\n",
    "        y_train.values.ravel(),\n",
    "        k_folds = 5,\n",
    "        metric = score)\n",
    "    return best_model, best_params, best_score\n",
    "\n",
    "def encode_features(df):\n",
    "    \n",
    "    mask_non_numeric = df.dtypes == object\n",
    "    mask_non_numeric = mask_non_numeric[mask_non_numeric]\n",
    "    unique_values = dataframe_utils.get_column_categories(df[mask_non_numeric.index],verbose = 0);\n",
    "\n",
    "    for col in unique_values.keys():\n",
    "        df.loc[df[col] == 'detected',col] = 1\n",
    "        df.loc[df[col] == 'not_detected',col] = 0\n",
    "\n",
    "    df.loc[df[class_label] == 'positive',class_label] = 1\n",
    "    df.loc[df[class_label] == 'negative',class_label] = 0\n",
    "\n",
    "    non_float_cols = (df.dtypes == np.float).values\n",
    "    for col in df.columns:\n",
    "        if df[col].dtypes == np.float:\n",
    "            continue\n",
    "        df[col] = df[col].astype(int)\n",
    "    return df;\n",
    "\n",
    "def prepare_data_for_model(df):\n",
    "\n",
    "    columns_x = df.drop(columns = [class_label]).columns\n",
    "    column_y = [class_label]\n",
    "\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "    df[columns_x], df[column_y],  test_size=0.20, random_state=42)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(df[columns_x])\n",
    "\n",
    "    X_train_norm = pd.DataFrame(data=scaler.transform(X_train),columns=X_train.columns)\n",
    "    X_validation_norm = pd.DataFrame(data=scaler.transform(X_validation),columns=X_train.columns)\n",
    "    return X_train_norm, X_validation_norm, y_train, y_validation\n",
    "\n",
    "\n",
    "def drop_by_data_threshold(df,threshold,axis):\n",
    "\n",
    "    if axis == 'row': axis = 1\n",
    "    elif axis == 'col' or axis == 'column': axis = 0\n",
    "\n",
    "    max_values = df.shape[axis]\n",
    "    value_threshold = int(threshold * max_values)\n",
    "    mask_values_to_keep = df.count(axis = axis) >= value_threshold\n",
    "    # print('threshold = ',value_threshold)\n",
    "    if axis == 1: return df[mask_values_to_keep]\n",
    "    elif axis == 0: return df.loc[:,mask_values_to_keep.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O resultado final não foi satisfatório na maioria das vezes. Apenas uma vez que eu consegui um f1 de 0.699. Foi o maior até então.\n",
    "\n",
    "# busca exaustiva\n",
    "def grid_search(*args):\n",
    "\n",
    "    itens = [x for x in args]\n",
    "    list_size = len(itens)\n",
    "    current = 0\n",
    "\n",
    "    threshold1 = np.arange(0, 1.05, 0.05)\n",
    "    threshold2 = np.arange(0, 1.05, 0.05)\n",
    "    threshold3 = np.arange(0, 1.05, 0.05)\n",
    "\n",
    "    options = np.meshgrid(threshold1, threshold2, threshold3, sparse=False)\n",
    "\n",
    "    comb = np.array(np.meshgrid(x, y, z)).T.reshape(-1,3)\n",
    "    comb.shape"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37764bitdefaultvenv42fbb5f68c3a497e94ae30fe23e8bc4c",
   "display_name": "Python 3.7.7 64-bit ('default': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}